# TACAN_video_reid

This is the code repository for our tech paper "Temporal Aggregation with Clip-level Attention for Video-based Person Re-identification": https://ieeexplore.ieee.org/document/9093413, 
poster video:https://www.youtube.com/watch?v=PhWGC67pTsA. If you find this helpful for your research, please cit
```
@INPROCEEDINGS{9093413,
  author={M. {Li} and H. {Xu} and J. {Wang} and W. {Li} and Y. {Sun}},
  booktitle={2020 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Temporal Aggregation with Clip-level Attention for Video-based Person Re-identification}, 
  year={2020},
  volume={},
  number={},
  pages={3365-3373},
  keywords={Feature extraction;Training;Task analysis;Measurement;Robustness;Computational modeling;Aggregates},
  doi={10.1109/WACV45572.2020.9093413},
  ISSN={2642-9381},
  month={March},}
```
  
### Introduction
Originl Codes are forked from: [video-person-reid](https://github.com/jiyanggao/Video-Person-ReID) and [deep-person-reid](https://github.com/KaiyangZhou/deep-person-reid), thanks for their sharing.

Based on that, we implement: <br>
(1) Original temporal modeling methods including temporal pooling, temporal attention, RNN and 3D conv. Some other video-based reid method: Quality Aware, Intra-clip Aggregation, Clip-level Weight and MGN-based model are implemented.<br>
(2) The base loss function and basic training framework remain the same as [deep-person-reid](https://github.com/KaiyangZhou/deep-person-reid). <br>
(3) Center Loss and Min-max Loss was implemented. <br>
(4) Other tuning strategies: learning rate warmup and data augmentation (synchonized in same clips). <br>
(5) Visualize training process by utilizing Tensorboard. <br>
(6) Compatibility work is done. **PyTorch 1.0.1, Torchvision 0.2.2 and Python 3.6** is now compatable.


### Dataset
All experiments are done on MARS, as it is the largest dataset available to date for video-based person reID. Please follow [deep-person-reid](https://github.com/KaiyangZhou/deep-person-reid) to prepare the data. The instructions are copied here:

1. Create a directory named `mars/` under `data/`.
2. Download dataset to `data/mars/` from http://www.liangzheng.com.cn/Project/project_mars.html.
3. Extract `bbox_train.zip` and `bbox_test.zip`.
4. Download split information from https://github.com/liangzheng06/MARS-evaluation/tree/master/info and put `info/` in `data/mars`. The data structure would look like:
```
mars/
    bbox_test/
    bbox_train/
    info/
```
5. Use `-d mars` when running the training code.

If new data need to be trained on, filenames for the extra data must be in the same format as MARS and they seperated into training and testing according to the person IDs. Then data information files in the folder `info/` need to be updated.

The `data_ext/update_info_files.py` is the script to update data information files. For the training set, `train_name.txt` and `tracks_train_info.mat` files in `info/` need to be updated while for the testing set, `query_IDX.mat`, `test_name.txt` and `tracks_test_info.mat` need to be updated. If the extra data will be extended after the orignial data, the "update_\*" functions in the script will be utilized. If some certain data are independently used, then the information files are re-generated by the "generate_\*" functions.

### Usage
To train the model, please run

    sh run.sh
In `run.sh`, --arch defines which model to use. In `models/__init__.py`, names for different model types can be found.

The first command in `run.sh` can give out the best performance on MARS without changing any parameters. The training process will last for 800 epochs with the validation testing for every 50 epochs.


An `./result/` folder will be generated when training. To visualize training process and accuracy trend, please run

    tensorboard --logdir=./result
An HTTP link will be shown on terminal and you could see different charts on webpage browser using this link.

### 中文版
这是我们的技术论文“片段级别时间注意聚合的视频行人再识别算法”的代码库，论文地址：https://ieeexplore.ieee.org/document/9093413，海报视频：https://www.youtube.com/watch?v=PhWGC67pTsA。 如果您发现这对您的研究有所帮助，请引用
```
@INPROCEEDINGS{9093413,
  author={M. {Li} and H. {Xu} and J. {Wang} and W. {Li} and Y. {Sun}},
  booktitle={2020 IEEE Winter Conference on Applications of Computer Vision (WACV)}, 
  title={Temporal Aggregation with Clip-level Attention for Video-based Person Re-identification}, 
  year={2020},
  volume={},
  number={},
  pages={3365-3373},
  keywords={Feature extraction;Training;Task analysis;Measurement;Robustness;Computational modeling;Aggregates},
  doi={10.1109/WACV45572.2020.9093413},
  ISSN={2642-9381},
  month={March},}
```
### 项目介绍
代码派生于[video-person-reid](https://github.com/jiyanggao/Video-Person-ReID) 和 [deep-person-reid](https://github.com/KaiyangZhou/deep-person-reid)，感谢作者的开源分享。
基于开源代码，我们的代码包含：<br>
（1）片段间的时序平均聚合、时序注意聚合、RNN聚合、3D卷积，片段间的注意聚合、MGN等；<br>
（2）基础训练框架和基础损失函数与[deep-person-reid](https://github.com/KaiyangZhou/deep-person-reid)大体相同；<br>
（3）增加了论文中提到的Min-max loss模块；<br>
（4）其他调整策略：学习率预热和数据增强（在同一剪辑中同步化）；<br>
（5）使用Tensorboard可视化训练过程；<br>
（6）完成兼容性工作。 代码现在可以兼容PyTorch 1.0.1，Torchvision 0.2.2和Python 3.6。<br>
